{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lpcTL4f2G9WE"
      },
      "source": [
        "## Installation of libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PM2h7czt2s0f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c34f22f8-4c62-4a24-a088-8bcdb5de7b24"
      },
      "source": [
        "!pip install --quiet transformers==4.5.0\n",
        "!pip install --quiet sentencepiece==0.1.95\n",
        "!pip install --quiet textwrap3==0.9.2\n",
        "!pip install --quiet nltk==3.2.5"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 9.98 s (started: 2021-05-20 17:00:04 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDHhiuKJ2eva",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd948c3b-bfa8-4311-8e1a-8214fc5ae88e"
      },
      "source": [
        "!pip install --quiet ipython-autotime\n",
        "%load_ext autotime"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autotime extension is already loaded. To reload it, use:\n",
            "  %reload_ext autotime\n",
            "time: 2.5 s (started: 2021-05-20 17:00:14 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords"
      ],
      "metadata": {
        "id": "EbwZZMWBYO3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EtWZC2P2YOyq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iDxEsifVYOvI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Extractive Summarization using Tf-idf"
      ],
      "metadata": {
        "id": "Lxb6EnLoYtKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "\n",
        "\n",
        "# Tokenization and preprocessing\n",
        "sentences = nltk.sent_tokenize(text)\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "\n",
        "# TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words=list(stop_words))\n",
        "tfidf_matrix = vectorizer.fit_transform(sentences)\n",
        "\n",
        "# Compute cosine similarity between sentences\n",
        "similarity_matrix = cosine_similarity(tfidf_matrix, tfidf_matrix)\n",
        "\n",
        "# Sentence ranking (using average TF-IDF scores)\n",
        "sentence_scores = similarity_matrix.mean(axis=1)\n",
        "\n",
        "# Sort sentences by score and extract top sentences for summarization\n",
        "num_sentences = 3  # Adjust the number of sentences for the summary\n",
        "top_sentence_indices = sentence_scores.argsort()[-num_sentences:][::-1]\n",
        "summary_sentences = [sentences[i] for i in top_sentence_indices]\n",
        "\n",
        "# Print the extractive summary\n",
        "print(\"Extractive Summary:\")\n",
        "for sentence in summary_sentences:\n",
        "    print(\"-\", sentence)\n"
      ],
      "metadata": {
        "id": "XQWdCbURXwMj",
        "outputId": "6d26fc1a-f822-4f12-c8f0-32d2ff2e6831",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extractive Summary:\n",
            "- In a recent tweet,\n",
            "Musk put out a statement from Tesla that it was “concerned” about the rapidly increasing use of fossil fuels for Bitcoin (price in India) mining and\n",
            "transaction, and hence was suspending vehicle purchases using the cryptocurrency.\n",
            "- After saying that his electric vehicle-making company\n",
            "Tesla will not accept payments in Bitcoin because of environmental concerns, he tweeted that he was working with developers of Dogecoin to improve\n",
            "system transaction efficiency.\n",
            "- The SpaceX CEO has in recent months often tweeted in support of Dogecoin, but rarely for Bitcoin.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "summary = '\\n'.join(summary_sentences)\n"
      ],
      "metadata": {
        "id": "SCdtpBI-ZXbG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G2EgRecwXwIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HCz_D3l3XwEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6HsNh7aLXwAD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K2s6ZTxCXv8L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "USING lstm"
      ],
      "metadata": {
        "id": "AtscKfPdbRu0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install textwrap3"
      ],
      "metadata": {
        "id": "xiyVjckiX9eD",
        "outputId": "7d9d53e2-bcdf-4e27-f2c2-7407988cf293",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting textwrap3\n",
            "  Downloading textwrap3-0.9.2-py2.py3-none-any.whl (12 kB)\n",
            "Installing collected packages: textwrap3\n",
            "Successfully installed textwrap3-0.9.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, LSTM, Dense\n",
        "\n",
        "# Sample text data\n",
        "text = summary\n",
        "\n",
        "# Tokenization and preprocessing\n",
        "tokens = text.split()\n",
        "vocab = sorted(set(tokens))\n",
        "word_to_index = {word: index for index, word in enumerate(vocab)}\n",
        "index_to_word = {index: word for word, index in word_to_index.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Generate training sequences\n",
        "max_sequence_length = 50\n",
        "sequences = []\n",
        "for i in range(max_sequence_length, len(tokens)):\n",
        "    sequence = tokens[i - max_sequence_length:i]\n",
        "    sequences.append(sequence)\n",
        "\n",
        "# Create input and target data\n",
        "X = []\n",
        "y = []\n",
        "for sequence in sequences:\n",
        "    X.append([word_to_index[word] for word in sequence[:-1]])\n",
        "    y.append(word_to_index[sequence[-1]])\n",
        "X = np.array(X)\n",
        "y = np.array(y)\n",
        "\n",
        "# Define LSTM model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=vocab_size, output_dim=50, input_length=max_sequence_length-1),\n",
        "    LSTM(100),\n",
        "    Dense(vocab_size, activation='softmax')\n",
        "])\n",
        "\n",
        "# Compile model\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam')\n",
        "\n",
        "# Train model\n",
        "model.fit(X, y, epochs=100, verbose=2)\n",
        "\n",
        "# Function to generate question and answer\n",
        "def generate_question_and_answer(text):\n",
        "    # Tokenization and preprocessing\n",
        "    input_sequence = text.split()[-(max_sequence_length-1):]\n",
        "    input_sequence = [word_to_index.get(word, 0) for word in input_sequence]\n",
        "\n",
        "    # Generate next word using trained model\n",
        "    predicted_index = np.argmax(model.predict(np.array([input_sequence]))[0])\n",
        "    predicted_word = index_to_word.get(predicted_index, \"<UNK>\")\n",
        "\n",
        "    # Generate question and answer\n",
        "    question = f\"What is the {predicted_word.capitalize()}?\"\n",
        "    answer = f\"The {predicted_word} is...\"\n",
        "\n",
        "    return question, answer\n",
        "\n",
        "# Generate question and answer\n",
        "question, answer = generate_question_and_answer(text)\n",
        "print(\"Question:\", question)\n",
        "print(\"Answer:\", answer)\n"
      ],
      "metadata": {
        "id": "aritQsrlX9as",
        "outputId": "8676d64b-ea2b-4c46-ff3e-f8f9a0d09db9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "2/2 - 3s - loss: 4.2631 - 3s/epoch - 2s/step\n",
            "Epoch 2/100\n",
            "2/2 - 0s - loss: 4.2537 - 253ms/epoch - 126ms/step\n",
            "Epoch 3/100\n",
            "2/2 - 0s - loss: 4.2442 - 21ms/epoch - 11ms/step\n",
            "Epoch 4/100\n",
            "2/2 - 0s - loss: 4.2332 - 251ms/epoch - 126ms/step\n",
            "Epoch 5/100\n",
            "2/2 - 0s - loss: 4.2153 - 464ms/epoch - 232ms/step\n",
            "Epoch 6/100\n",
            "2/2 - 0s - loss: 4.1808 - 262ms/epoch - 131ms/step\n",
            "Epoch 7/100\n",
            "2/2 - 0s - loss: 4.0922 - 25ms/epoch - 12ms/step\n",
            "Epoch 8/100\n",
            "2/2 - 0s - loss: 3.9129 - 20ms/epoch - 10ms/step\n",
            "Epoch 9/100\n",
            "2/2 - 0s - loss: 3.8364 - 22ms/epoch - 11ms/step\n",
            "Epoch 10/100\n",
            "2/2 - 0s - loss: 3.8073 - 18ms/epoch - 9ms/step\n",
            "Epoch 11/100\n",
            "2/2 - 0s - loss: 3.7678 - 21ms/epoch - 11ms/step\n",
            "Epoch 12/100\n",
            "2/2 - 0s - loss: 3.7264 - 19ms/epoch - 9ms/step\n",
            "Epoch 13/100\n",
            "2/2 - 0s - loss: 3.6895 - 19ms/epoch - 10ms/step\n",
            "Epoch 14/100\n",
            "2/2 - 0s - loss: 3.6519 - 20ms/epoch - 10ms/step\n",
            "Epoch 15/100\n",
            "2/2 - 0s - loss: 3.6200 - 20ms/epoch - 10ms/step\n",
            "Epoch 16/100\n",
            "2/2 - 0s - loss: 3.5996 - 21ms/epoch - 11ms/step\n",
            "Epoch 17/100\n",
            "2/2 - 0s - loss: 3.5780 - 20ms/epoch - 10ms/step\n",
            "Epoch 18/100\n",
            "2/2 - 0s - loss: 3.5634 - 22ms/epoch - 11ms/step\n",
            "Epoch 19/100\n",
            "2/2 - 0s - loss: 3.5479 - 19ms/epoch - 10ms/step\n",
            "Epoch 20/100\n",
            "2/2 - 0s - loss: 3.5348 - 18ms/epoch - 9ms/step\n",
            "Epoch 21/100\n",
            "2/2 - 0s - loss: 3.5198 - 248ms/epoch - 124ms/step\n",
            "Epoch 22/100\n",
            "2/2 - 0s - loss: 3.5113 - 25ms/epoch - 13ms/step\n",
            "Epoch 23/100\n",
            "2/2 - 0s - loss: 3.5011 - 20ms/epoch - 10ms/step\n",
            "Epoch 24/100\n",
            "2/2 - 0s - loss: 3.4936 - 17ms/epoch - 9ms/step\n",
            "Epoch 25/100\n",
            "2/2 - 0s - loss: 3.4887 - 254ms/epoch - 127ms/step\n",
            "Epoch 26/100\n",
            "2/2 - 0s - loss: 3.4843 - 21ms/epoch - 11ms/step\n",
            "Epoch 27/100\n",
            "2/2 - 0s - loss: 3.4780 - 162ms/epoch - 81ms/step\n",
            "Epoch 28/100\n",
            "2/2 - 0s - loss: 3.4705 - 162ms/epoch - 81ms/step\n",
            "Epoch 29/100\n",
            "2/2 - 0s - loss: 3.4635 - 17ms/epoch - 8ms/step\n",
            "Epoch 30/100\n",
            "2/2 - 0s - loss: 3.4592 - 180ms/epoch - 90ms/step\n",
            "Epoch 31/100\n",
            "2/2 - 0s - loss: 3.4561 - 32ms/epoch - 16ms/step\n",
            "Epoch 32/100\n",
            "2/2 - 0s - loss: 3.4535 - 23ms/epoch - 11ms/step\n",
            "Epoch 33/100\n",
            "2/2 - 0s - loss: 3.4437 - 20ms/epoch - 10ms/step\n",
            "Epoch 34/100\n",
            "2/2 - 0s - loss: 3.4371 - 32ms/epoch - 16ms/step\n",
            "Epoch 35/100\n",
            "2/2 - 0s - loss: 3.4308 - 34ms/epoch - 17ms/step\n",
            "Epoch 36/100\n",
            "2/2 - 0s - loss: 3.4257 - 35ms/epoch - 18ms/step\n",
            "Epoch 37/100\n",
            "2/2 - 0s - loss: 3.4208 - 38ms/epoch - 19ms/step\n",
            "Epoch 38/100\n",
            "2/2 - 0s - loss: 3.4184 - 21ms/epoch - 10ms/step\n",
            "Epoch 39/100\n",
            "2/2 - 0s - loss: 3.4128 - 19ms/epoch - 9ms/step\n",
            "Epoch 40/100\n",
            "2/2 - 0s - loss: 3.4115 - 19ms/epoch - 10ms/step\n",
            "Epoch 41/100\n",
            "2/2 - 0s - loss: 3.4064 - 27ms/epoch - 14ms/step\n",
            "Epoch 42/100\n",
            "2/2 - 0s - loss: 3.4027 - 33ms/epoch - 17ms/step\n",
            "Epoch 43/100\n",
            "2/2 - 0s - loss: 3.3983 - 30ms/epoch - 15ms/step\n",
            "Epoch 44/100\n",
            "2/2 - 0s - loss: 3.3877 - 26ms/epoch - 13ms/step\n",
            "Epoch 45/100\n",
            "2/2 - 0s - loss: 3.3791 - 42ms/epoch - 21ms/step\n",
            "Epoch 46/100\n",
            "2/2 - 0s - loss: 3.3691 - 30ms/epoch - 15ms/step\n",
            "Epoch 47/100\n",
            "2/2 - 0s - loss: 3.3556 - 19ms/epoch - 10ms/step\n",
            "Epoch 48/100\n",
            "2/2 - 0s - loss: 3.3403 - 27ms/epoch - 13ms/step\n",
            "Epoch 49/100\n",
            "2/2 - 0s - loss: 3.3246 - 21ms/epoch - 10ms/step\n",
            "Epoch 50/100\n",
            "2/2 - 0s - loss: 3.3148 - 24ms/epoch - 12ms/step\n",
            "Epoch 51/100\n",
            "2/2 - 0s - loss: 3.3060 - 22ms/epoch - 11ms/step\n",
            "Epoch 52/100\n",
            "2/2 - 0s - loss: 3.2975 - 23ms/epoch - 12ms/step\n",
            "Epoch 53/100\n",
            "2/2 - 0s - loss: 3.2798 - 18ms/epoch - 9ms/step\n",
            "Epoch 54/100\n",
            "2/2 - 0s - loss: 3.2530 - 19ms/epoch - 9ms/step\n",
            "Epoch 55/100\n",
            "2/2 - 0s - loss: 3.2207 - 20ms/epoch - 10ms/step\n",
            "Epoch 56/100\n",
            "2/2 - 0s - loss: 3.1862 - 24ms/epoch - 12ms/step\n",
            "Epoch 57/100\n",
            "2/2 - 0s - loss: 3.1658 - 22ms/epoch - 11ms/step\n",
            "Epoch 58/100\n",
            "2/2 - 0s - loss: 3.1301 - 23ms/epoch - 12ms/step\n",
            "Epoch 59/100\n",
            "2/2 - 0s - loss: 3.1001 - 19ms/epoch - 10ms/step\n",
            "Epoch 60/100\n",
            "2/2 - 0s - loss: 3.0529 - 22ms/epoch - 11ms/step\n",
            "Epoch 61/100\n",
            "2/2 - 0s - loss: 3.0018 - 21ms/epoch - 11ms/step\n",
            "Epoch 62/100\n",
            "2/2 - 0s - loss: 2.9581 - 20ms/epoch - 10ms/step\n",
            "Epoch 63/100\n",
            "2/2 - 0s - loss: 2.9127 - 283ms/epoch - 141ms/step\n",
            "Epoch 64/100\n",
            "2/2 - 0s - loss: 2.8734 - 18ms/epoch - 9ms/step\n",
            "Epoch 65/100\n",
            "2/2 - 0s - loss: 2.8386 - 17ms/epoch - 8ms/step\n",
            "Epoch 66/100\n",
            "2/2 - 0s - loss: 2.7919 - 15ms/epoch - 8ms/step\n",
            "Epoch 67/100\n",
            "2/2 - 0s - loss: 2.7505 - 16ms/epoch - 8ms/step\n",
            "Epoch 68/100\n",
            "2/2 - 0s - loss: 2.6958 - 16ms/epoch - 8ms/step\n",
            "Epoch 69/100\n",
            "2/2 - 0s - loss: 2.6340 - 15ms/epoch - 7ms/step\n",
            "Epoch 70/100\n",
            "2/2 - 0s - loss: 2.5797 - 18ms/epoch - 9ms/step\n",
            "Epoch 71/100\n",
            "2/2 - 0s - loss: 2.5437 - 162ms/epoch - 81ms/step\n",
            "Epoch 72/100\n",
            "2/2 - 0s - loss: 2.4951 - 17ms/epoch - 8ms/step\n",
            "Epoch 73/100\n",
            "2/2 - 0s - loss: 2.4527 - 33ms/epoch - 17ms/step\n",
            "Epoch 74/100\n",
            "2/2 - 0s - loss: 2.4075 - 28ms/epoch - 14ms/step\n",
            "Epoch 75/100\n",
            "2/2 - 0s - loss: 2.3701 - 19ms/epoch - 10ms/step\n",
            "Epoch 76/100\n",
            "2/2 - 0s - loss: 2.3272 - 30ms/epoch - 15ms/step\n",
            "Epoch 77/100\n",
            "2/2 - 0s - loss: 2.2758 - 24ms/epoch - 12ms/step\n",
            "Epoch 78/100\n",
            "2/2 - 0s - loss: 2.2311 - 26ms/epoch - 13ms/step\n",
            "Epoch 79/100\n",
            "2/2 - 0s - loss: 2.2130 - 26ms/epoch - 13ms/step\n",
            "Epoch 80/100\n",
            "2/2 - 0s - loss: 2.1623 - 27ms/epoch - 13ms/step\n",
            "Epoch 81/100\n",
            "2/2 - 0s - loss: 2.1284 - 29ms/epoch - 14ms/step\n",
            "Epoch 82/100\n",
            "2/2 - 0s - loss: 2.1081 - 31ms/epoch - 16ms/step\n",
            "Epoch 83/100\n",
            "2/2 - 0s - loss: 2.0568 - 36ms/epoch - 18ms/step\n",
            "Epoch 84/100\n",
            "2/2 - 0s - loss: 2.0227 - 43ms/epoch - 21ms/step\n",
            "Epoch 85/100\n",
            "2/2 - 0s - loss: 1.9763 - 46ms/epoch - 23ms/step\n",
            "Epoch 86/100\n",
            "2/2 - 0s - loss: 1.9550 - 28ms/epoch - 14ms/step\n",
            "Epoch 87/100\n",
            "2/2 - 0s - loss: 1.9076 - 41ms/epoch - 21ms/step\n",
            "Epoch 88/100\n",
            "2/2 - 0s - loss: 1.9084 - 62ms/epoch - 31ms/step\n",
            "Epoch 89/100\n",
            "2/2 - 0s - loss: 1.8569 - 27ms/epoch - 14ms/step\n",
            "Epoch 90/100\n",
            "2/2 - 0s - loss: 1.8329 - 43ms/epoch - 21ms/step\n",
            "Epoch 91/100\n",
            "2/2 - 0s - loss: 1.8117 - 20ms/epoch - 10ms/step\n",
            "Epoch 92/100\n",
            "2/2 - 0s - loss: 1.7681 - 38ms/epoch - 19ms/step\n",
            "Epoch 93/100\n",
            "2/2 - 0s - loss: 1.7378 - 35ms/epoch - 17ms/step\n",
            "Epoch 94/100\n",
            "2/2 - 0s - loss: 1.7202 - 42ms/epoch - 21ms/step\n",
            "Epoch 95/100\n",
            "2/2 - 0s - loss: 1.6949 - 39ms/epoch - 19ms/step\n",
            "Epoch 96/100\n",
            "2/2 - 0s - loss: 1.6678 - 28ms/epoch - 14ms/step\n",
            "Epoch 97/100\n",
            "2/2 - 0s - loss: 1.6503 - 36ms/epoch - 18ms/step\n",
            "Epoch 98/100\n",
            "2/2 - 0s - loss: 1.6293 - 27ms/epoch - 13ms/step\n",
            "Epoch 99/100\n",
            "2/2 - 0s - loss: 1.5914 - 36ms/epoch - 18ms/step\n",
            "Epoch 100/100\n",
            "2/2 - 0s - loss: 1.5712 - 37ms/epoch - 18ms/step\n",
            "1/1 [==============================] - 0s 348ms/step\n",
            "Question: What is the For?\n",
            "Answer: The for is...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PjLkPRmoX9XW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zt5wyqeq0Pq8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "41e4037f-88bf-4bca-8108-c1e06f27d67a"
      },
      "source": [
        "from textwrap3 import wrap\n",
        "\n",
        "text = \"\"\"Elon Musk has shown again he can influence the digital currency market with just his tweets. After saying that his electric vehicle-making company\n",
        "Tesla will not accept payments in Bitcoin because of environmental concerns, he tweeted that he was working with developers of Dogecoin to improve\n",
        "system transaction efficiency. Following the two distinct statements from him, the world's largest cryptocurrency hit a two-month low, while Dogecoin\n",
        "rallied by about 20 percent. The SpaceX CEO has in recent months often tweeted in support of Dogecoin, but rarely for Bitcoin.  In a recent tweet,\n",
        "Musk put out a statement from Tesla that it was “concerned” about the rapidly increasing use of fossil fuels for Bitcoin (price in India) mining and\n",
        "transaction, and hence was suspending vehicle purchases using the cryptocurrency.  A day later he again tweeted saying, “To be clear, I strongly\n",
        "believe in crypto, but it can't drive a massive increase in fossil fuel use, especially coal”.  It triggered a downward spiral for Bitcoin value but\n",
        "the cryptocurrency has stabilised since.   A number of Twitter users welcomed Musk's statement. One of them said it's time people started realising\n",
        "that Dogecoin “is here to stay” and another referred to Musk's previous assertion that crypto could become the world's future currency.\"\"\"\n",
        "\n",
        "for wrp in wrap(text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Elon Musk has shown again he can influence the digital currency market with just his tweets. After saying that his electric vehicle-making company\n",
            "Tesla will not accept payments in Bitcoin because of environmental concerns, he tweeted that he was working with developers of Dogecoin to improve\n",
            "system transaction efficiency. Following the two distinct statements from him, the world's largest cryptocurrency hit a two-month low, while Dogecoin\n",
            "rallied by about 20 percent. The SpaceX CEO has in recent months often tweeted in support of Dogecoin, but rarely for Bitcoin.  In a recent tweet,\n",
            "Musk put out a statement from Tesla that it was “concerned” about the rapidly increasing use of fossil fuels for Bitcoin (price in India) mining and\n",
            "transaction, and hence was suspending vehicle purchases using the cryptocurrency.  A day later he again tweeted saying, “To be clear, I strongly\n",
            "believe in crypto, but it can't drive a massive increase in fossil fuel use, especially coal”.  It triggered a downward spiral for Bitcoin value but\n",
            "the cryptocurrency has stabilised since.   A number of Twitter users welcomed Musk's statement. One of them said it's time people started realising\n",
            "that Dogecoin “is here to stay” and another referred to Musk's previous assertion that crypto could become the world's future currency.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imbR470g15Fq"
      },
      "source": [
        "# **Summarization with T5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cXs7fnvCarm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dcc1982a-7301-4bf8-a0d5-241d1872f6e6"
      },
      "source": [
        "import torch\n",
        "from transformers import T5ForConditionalGeneration,T5Tokenizer\n",
        "summary_model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
        "summary_tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "summary_model = summary_model.to(device)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 20.9 s (started: 2021-05-20 17:02:02 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SzGsTUJ8TyAN",
        "outputId": "7742b4f6-7aad-422a-9b9d-6d033ebc3eb6"
      },
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def set_seed(seed: int):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "set_seed(42)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 3.71 ms (started: 2021-05-20 17:02:25 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JaEy5Xw_UMf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66dce004-255d-4741-c505-c22a3d78dede"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('brown')\n",
        "nltk.download('wordnet')\n",
        "from nltk.corpus import wordnet as wn\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "def postprocesstext (content):\n",
        "  final=\"\"\n",
        "  for sent in sent_tokenize(content):\n",
        "    sent = sent.capitalize()\n",
        "    final = final +\" \"+sent\n",
        "  return final\n",
        "\n",
        "\n",
        "def summarizer(text,model,tokenizer):\n",
        "  text = text.strip().replace(\"\\n\",\" \")\n",
        "  text = \"summarize: \"+text\n",
        "  # print (text)\n",
        "  max_len = 512\n",
        "  encoding = tokenizer.encode_plus(text,max_length=max_len, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
        "\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = model.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=3,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  min_length = 75,\n",
        "                                  max_length=300)\n",
        "\n",
        "\n",
        "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "  summary = dec[0]\n",
        "  summary = postprocesstext(summary)\n",
        "  summary= summary.strip()\n",
        "\n",
        "  return summary\n",
        "\n",
        "\n",
        "summarized_text = summarizer(text,summary_model,summary_tokenizer)\n",
        "\n",
        "\n",
        "print (\"\\noriginal Text >>\")\n",
        "for wrp in wrap(text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")\n",
        "print (\"Summarized Text >>\")\n",
        "for wrp in wrap(summarized_text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]   Package brown is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "\n",
            "original Text >>\n",
            "Elon Musk has shown again he can influence the digital currency market with just his tweets. After saying that his electric vehicle-making company\n",
            "Tesla will not accept payments in Bitcoin because of environmental concerns, he tweeted that he was working with developers of Dogecoin to improve\n",
            "system transaction efficiency. Following the two distinct statements from him, the world's largest cryptocurrency hit a two-month low, while Dogecoin\n",
            "rallied by about 20 percent. The SpaceX CEO has in recent months often tweeted in support of Dogecoin, but rarely for Bitcoin.  In a recent tweet,\n",
            "Musk put out a statement from Tesla that it was “concerned” about the rapidly increasing use of fossil fuels for Bitcoin (price in India) mining and\n",
            "transaction, and hence was suspending vehicle purchases using the cryptocurrency.  A day later he again tweeted saying, “To be clear, I strongly\n",
            "believe in crypto, but it can't drive a massive increase in fossil fuel use, especially coal”.  It triggered a downward spiral for Bitcoin value but\n",
            "the cryptocurrency has stabilised since.   A number of Twitter users welcomed Musk's statement. One of them said it's time people started realising\n",
            "that Dogecoin “is here to stay” and another referred to Musk's previous assertion that crypto could become the world's future currency.\n",
            "\n",
            "\n",
            "Summarized Text >>\n",
            "Musk tweeted that his electric vehicle-making company tesla will not accept payments in bitcoin because of environmental concerns. He also said that\n",
            "the company was working with developers of dogecoin to improve system transaction efficiency. The world's largest cryptocurrency hit a two-month low,\n",
            "while doge coin rallied by about 20 percent. Musk has in recent months often tweeted in support of crypto, but rarely for bitcoin.\n",
            "\n",
            "\n",
            "time: 1.89 s (started: 2021-05-20 17:02:29 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N0YrWTxQCo9q"
      },
      "source": [
        "# **Answer Span Extraction (Keywords and Noun Phrases)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "r9TmL5LR2Ucg",
        "outputId": "8f38b092-f2aa-4f2b-c6a9-7a259dccaadd"
      },
      "source": [
        "!pip install --quiet git+https://github.com/boudinfl/pke.git@dc4d5f21e0ffe64c4df93c46146d29d1c522476b\n",
        "!pip install --quiet flashtext==2.7"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  Building wheel for pke (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "time: 11.8 s (started: 2021-05-20 17:04:38 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84DxJGFn4MfD"
      },
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "from nltk.corpus import stopwords\n",
        "import string\n",
        "import pke\n",
        "import traceback\n",
        "\n",
        "def get_nouns_multipartite(content):\n",
        "    out=[]\n",
        "    try:\n",
        "        extractor = pke.unsupervised.MultipartiteRank()\n",
        "        extractor.load_document(input=content)\n",
        "        #    not contain punctuation marks or stopwords as candidates.\n",
        "        pos = {'PROPN','NOUN'}\n",
        "        #pos = {'PROPN','NOUN'}\n",
        "        stoplist = list(string.punctuation)\n",
        "        stoplist += ['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n",
        "        stoplist += stopwords.words('english')\n",
        "        extractor.candidate_selection(pos=pos, stoplist=stoplist)\n",
        "        # 4. build the Multipartite graph and rank candidates using random walk,\n",
        "        #    alpha controls the weight adjustment mechanism, see TopicRank for\n",
        "        #    threshold/method parameters.\n",
        "        extractor.candidate_weighting(alpha=1.1,\n",
        "                                      threshold=0.75,\n",
        "                                      method='average')\n",
        "        keyphrases = extractor.get_n_best(n=15)\n",
        "\n",
        "\n",
        "        for val in keyphrases:\n",
        "            out.append(val[0])\n",
        "    except:\n",
        "        out = []\n",
        "        traceback.print_exc()\n",
        "\n",
        "    return out"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J_sRWHAd4Wwp",
        "outputId": "46fa79f7-a526-4e0d-b120-84f350bdb151"
      },
      "source": [
        "from flashtext import KeywordProcessor\n",
        "\n",
        "\n",
        "def get_keywords(originaltext,summarytext):\n",
        "  keywords = get_nouns_multipartite(originaltext)\n",
        "  print (\"keywords unsummarized: \",keywords)\n",
        "  keyword_processor = KeywordProcessor()\n",
        "  for keyword in keywords:\n",
        "    keyword_processor.add_keyword(keyword)\n",
        "\n",
        "  keywords_found = keyword_processor.extract_keywords(summarytext)\n",
        "  keywords_found = list(set(keywords_found))\n",
        "  print (\"keywords_found in summarized: \",keywords_found)\n",
        "\n",
        "  important_keywords =[]\n",
        "  for keyword in keywords:\n",
        "    if keyword in keywords_found:\n",
        "      important_keywords.append(keyword)\n",
        "\n",
        "  return important_keywords[:4]\n",
        "\n",
        "\n",
        "imp_keywords = get_keywords(text,summarized_text)\n",
        "print (imp_keywords)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "keywords unsummarized:  ['elon musk', 'bitcoin', 'dogecoin', 'statements', 'tesla', 'tweets', 'cryptocurrency', 'vehicle', 'musk', 'system transaction efficiency', 'currency market', 'month low', 'fuels', 'company', 'world']\n",
            "keywords_found in summarized:  ['dogecoin', 'month low', 'world', 'tesla', 'company', 'system transaction efficiency', 'vehicle', 'cryptocurrency', 'musk', 'bitcoin']\n",
            "['bitcoin', 'dogecoin', 'tesla', 'cryptocurrency']\n",
            "time: 753 ms (started: 2021-05-20 17:05:48 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FXbq7b2WCaZ_"
      },
      "source": [
        "# **Question generation with T5**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3CuKlpL1Cj6C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5e744880-11a2-4274-db38-4ec7dceaff69"
      },
      "source": [
        "question_model = T5ForConditionalGeneration.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "question_tokenizer = T5Tokenizer.from_pretrained('ramsrigouthamg/t5_squad_v1')\n",
        "question_model = question_model.to(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "time: 8.2 s (started: 2021-05-20 17:08:47 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7uzA4uLJ_P48",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "845d9e57-74ac-43cc-ca8b-c6a6f13c3d35"
      },
      "source": [
        "def get_question(context,answer,model,tokenizer):\n",
        "  text = \"context: {} answer: {}\".format(context,answer)\n",
        "  encoding = tokenizer.encode_plus(text,max_length=384, pad_to_max_length=False,truncation=True, return_tensors=\"pt\").to(device)\n",
        "  input_ids, attention_mask = encoding[\"input_ids\"], encoding[\"attention_mask\"]\n",
        "\n",
        "  outs = model.generate(input_ids=input_ids,\n",
        "                                  attention_mask=attention_mask,\n",
        "                                  early_stopping=True,\n",
        "                                  num_beams=5,\n",
        "                                  num_return_sequences=1,\n",
        "                                  no_repeat_ngram_size=2,\n",
        "                                  max_length=72)\n",
        "\n",
        "\n",
        "  dec = [tokenizer.decode(ids,skip_special_tokens=True) for ids in outs]\n",
        "\n",
        "\n",
        "  Question = dec[0].replace(\"question:\",\"\")\n",
        "  Question= Question.strip()\n",
        "  return Question\n",
        "\n",
        "\n",
        "\n",
        "for wrp in wrap(summarized_text, 150):\n",
        "  print (wrp)\n",
        "print (\"\\n\")\n",
        "\n",
        "for answer in imp_keywords:\n",
        "  ques = get_question(summarized_text,answer,question_model,question_tokenizer)\n",
        "  print (ques)\n",
        "  print (answer.capitalize())\n",
        "  print (\"\\n\")\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Musk tweeted that his electric vehicle-making company tesla will not accept payments in bitcoin because of environmental concerns. He also said that\n",
            "the company was working with developers of dogecoin to improve system transaction efficiency. The world's largest cryptocurrency hit a two-month low,\n",
            "while doge coin rallied by about 20 percent. Musk has in recent months often tweeted in support of crypto, but rarely for bitcoin.\n",
            "\n",
            "\n",
            "What cryptocurrency did Musk rarely tweet about?\n",
            "Bitcoin\n",
            "\n",
            "\n",
            "What did Musk say he was working with to improve system transaction efficiency?\n",
            "Dogecoin\n",
            "\n",
            "\n",
            "What company did Musk say would not accept bitcoin payments?\n",
            "Tesla\n",
            "\n",
            "\n",
            "What has Musk often tweeted in support of?\n",
            "Cryptocurrency\n",
            "\n",
            "\n",
            "time: 1.04 s (started: 2021-05-20 17:09:00 +00:00)\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}